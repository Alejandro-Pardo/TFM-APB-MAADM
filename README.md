# ğŸ”¬ Automatic API Analysis and Classification through Deep Learning

[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

> **Master's Thesis Research Project**  
> Universidad PolitÃ©cnica de Madrid (UPM)  
> Master of Science in Machine Learning and Big Data

## ğŸ“‹ Project Overview

This repository contains the complete codebase for a master's thesis focused on developing automated systems for API analysis and classification using deep learning techniques. The project involves scraping, parsing, and analyzing AWS API documentation to extract structured information for machine learning applications.

**Author:** Alejandro Pardo BascuÃ±ana  
**Supervisor:** Jorge Blasco AlÃ­s  
**Institution:** Universidad PolitÃ©cnica de Madrid (UPM)

## ğŸš€ Features

- **Automated AWS API Documentation Parsing**: Comprehensive scraping and parsing of AWS API documentation
- **Structured Data Extraction**: Extract method signatures, parameters, and return types
- **Machine Learning Pipeline**: Classification and analysis of API patterns
- **Robust Error Handling**: Checkpoint system for resumable processing
- **Data Visualization**: Statistical analysis and visualization of API patterns

## ğŸ“ Project Structure

```
TFM-APB-MAADM/
â”œâ”€â”€ ğŸ“‚ parsing/                    # Core parsing and scraping modules
â”‚   â”œâ”€â”€ ğŸ“‚ utils/                 # Utility functions and configurations
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration and logging setup
â”‚   â”‚   â”œâ”€â”€ checkpoint_manager.py # Progress tracking and resumption
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py       # Text cleaning utilities
â”‚   â”‚   â””â”€â”€ timeout.py            # Timeout utilities
â”‚   â”œâ”€â”€ ğŸ“‚ parsers/               # Parser modules
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”‚   â”œâ”€â”€ method_parser.py      # Individual method parsing logic
â”‚   â”‚   â”œâ”€â”€ service_parser.py     # Service documentation parsing
â”‚   â”‚   â””â”€â”€ service_url_parser.py # Service URL extraction
â”‚   â”œâ”€â”€ main.py                   # Main entry point for parsing
â”‚   â”œâ”€â”€ service_processor.py      # Service processing coordination
â”‚   â”œâ”€â”€ checkpoint.json           # Progress checkpoint data
â”‚   â””â”€â”€ README.md                 # Parsing module documentation
â”œâ”€â”€ ğŸ“‚ docs/                      # Documentation and parsed data
â”‚   â”œâ”€â”€ aws_api_urls.txt          # AWS API URLs
â”‚   â”œâ”€â”€ ğŸ“‚ methods/               # Extracted method information by service
â”‚   â””â”€â”€ ğŸ“‚ services/              # Service-specific data
â”œâ”€â”€ ğŸ“Š statistics/                # Statistical analysis
â”‚   â”œâ”€â”€ statistics.ipynb          # Statistical analysis notebook
â”‚   â””â”€â”€ unique_action_verbs.txt   # Unique action verbs found in APIs
â”œâ”€â”€ ğŸ“‚ embeddings/                # Embedding generation and analysis
â”‚   â””â”€â”€ embeddings.ipynb          # Embedding generation notebook
â”œâ”€â”€ ğŸ“‹ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ“„ LICENSE                    # GPL v3 License
â””â”€â”€ ğŸ“„ README.md                  # This file
```

## ğŸ› ï¸ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/TFM-APB-MAADM.git
   cd TFM-APB-MAADM
   ```

2. **Create a virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸš€ Usage

### AWS API Documentation Parsing

To start parsing AWS API documentation:

```bash
cd parsing
python main.py
```

The parser will:
- Scrape AWS API documentation
- Extract structured information about methods and parameters
- Save progress with checkpoint system for resumable processing
- Generate JSON files with parsed data

### Data Analysis

Open the Jupyter notebooks for analysis:

```bash
# Statistical analysis
jupyter notebook statistics/statistics.ipynb

# Embedding generation and analysis
jupyter notebook embeddings/embeddings.ipynb
```

## ğŸ“Š Data Processing Pipeline

1. **Web Scraping**: Automated extraction of AWS API documentation
2. **Data Cleaning**: Text preprocessing and normalization
3. **Feature Extraction**: Method signatures, parameters, and metadata
4. **Machine Learning**: Classification and pattern analysis
5. **Visualization**: Statistical insights and data exploration

## ğŸ”§ Configuration

The parsing system can be configured through `parsing/utils/config.py`:

- **Logging levels**: Adjust verbosity of output
- **Checkpoint settings**: Configure resumable processing
- **Output formats**: Customize data export formats
- **Timeout settings**: Configure request timeouts

## ğŸ“ˆ Results

This research contributes to:
- **Automated API Documentation Analysis**: Scalable parsing of large API documentation sets
- **Machine Learning for API Classification**: Novel approaches to API pattern recognition
- **Data-Driven API Insights**: Statistical analysis of API design patterns

## ğŸ¤ Contributing

As this is an academic research project, contributions are welcome for:
- Bug fixes and improvements
- Additional analysis techniques
- Documentation enhancements
- Code optimization

## ğŸ“„ License

This project is licensed under the GNU General Public License v3.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Academic Context

This work is part of a Master's thesis in the Machine Learning and Big Data program at Universidad PolitÃ©cnica de Madrid. The research focuses on applying deep learning techniques to automated API analysis and classification.

## ğŸ“ Contact

**Alejandro Pardo BascuÃ±ana**  
Master's Student - Machine Learning and Big Data  
Universidad PolitÃ©cnica de Madrid (UPM)

For questions about this research, please open an issue in this repository.

---

*This research was conducted as part of the Master of Science in Machine Learning and Big Data at Universidad PolitÃ©cnica de Madrid under the supervision of Jorge Blasco AlÃ­s.*
