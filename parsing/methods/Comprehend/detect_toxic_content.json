{
  "method_name": "detect_toxic_content",
  "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend/client/detect_toxic_content.html",
  "description": "Performs toxicity analysis on the list of text strings that you provide as input. The API response contains a results list that matches the size of the input list. For more information about toxicity detection, see Toxicity detection in the Amazon Comprehend Developer Guide.",
  "parameters": [
    {
      "name": "TextSegments",
      "type": "list",
      "required": true,
      "description": "A list of up to 10 text strings. Each string has a maximum size of 1 KB, and the maximum size of the list is 10 KB. (dict) --- One of the of text strings. Each string has a size limit of 1KB. The text content.",
      "nested_params": [
        {
          "name": "Text",
          "type": "string",
          "required": false,
          "description": "The text content.",
          "nested_params": []
        }
      ]
    },
    {
      "name": "LanguageCode",
      "type": "string",
      "required": true,
      "description": "The language of the input text. Currently, English is the only supported language.",
      "nested_params": []
    }
  ],
  "return_structure": [
    {
      "name": "",
      "type": "dict",
      "description": "",
      "nested_items": [
        {
          "name": "ResultList",
          "type": "list",
          "description": "Results of the content moderation analysis. Each entry in the results list contains a list of toxic content types identified in the text, along with a confidence score for each content type. The results list also includes a toxicity score for each entry in the results list.",
          "nested_items": [
            {
              "name": "",
              "type": "dict",
              "description": "",
              "nested_items": [
                {
                  "name": "Labels",
                  "type": "list",
                  "description": "Array of toxic content types identified in the string.",
                  "nested_items": [
                    {
                      "name": "",
                      "type": "dict",
                      "description": "",
                      "nested_items": [
                        {
                          "name": "Name",
                          "type": "string",
                          "description": "The name of the toxic content type.",
                          "nested_items": []
                        },
                        {
                          "name": "Score",
                          "type": "float",
                          "description": "Model confidence in the detected content type. Value range is zero to one, where one is highest confidence.",
                          "nested_items": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "name": "Toxicity",
                  "type": "float",
                  "description": "Overall toxicity score for the string. Value range is zero to one, where one is the highest confidence.",
                  "nested_items": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}